{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import des bibliothèques nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks, applications, metrics\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import albumentations as A\n",
    "from albumentations.core.composition import OneOf\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration pour la reproductibilité\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Vérifier la disponibilité du GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Préparation et exploration des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres globaux\n",
    "IMAGE_SIZE = (256, 256)  # Taille réduite pour l'entraînement rapide\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 30\n",
    "NUM_CLASSES = 8  # 8 classes pour la segmentation urbaine\n",
    "\n",
    "# Mapping des classes originales (34 classes) vers 8 classes\n",
    "# Selon la documentation de Cityscapes\n",
    "CLASS_MAPPING = {\n",
    "    0: 7,   # unlabeled -> background\n",
    "    1: 0,   # ego vehicle -> vehicle\n",
    "    2: 0,   # rectification border -> vehicle\n",
    "    3: 7,   # out of roi -> background\n",
    "    4: 7,   # static -> background\n",
    "    5: 7,   # dynamic -> background\n",
    "    6: 7,   # ground -> background\n",
    "    7: 1,   # road -> road\n",
    "    8: 2,   # sidewalk -> sidewalk\n",
    "    9: 7,   # parking -> background\n",
    "    10: 7,  # rail track -> background\n",
    "    11: 3,  # building -> building\n",
    "    12: 7,  # wall -> background\n",
    "    13: 7,  # fence -> background\n",
    "    14: 7,  # guard rail -> background\n",
    "    15: 7,  # bridge -> background\n",
    "    16: 7,  # tunnel -> background\n",
    "    17: 4,  # pole -> pole\n",
    "    18: 7,  # polegroup -> background\n",
    "    19: 5,  # traffic light -> traffic light\n",
    "    20: 6,  # traffic sign -> traffic sign\n",
    "    21: 3,  # vegetation -> building\n",
    "    22: 7,  # terrain -> background\n",
    "    23: 7,  # sky -> background\n",
    "    24: 0,  # person -> vehicle\n",
    "    25: 0,  # rider -> vehicle\n",
    "    26: 0,  # car -> vehicle\n",
    "    27: 0,  # truck -> vehicle\n",
    "    28: 0,  # bus -> vehicle\n",
    "    29: 7,  # caravan -> background\n",
    "    30: 7,  # trailer -> background\n",
    "    31: 0,  # train -> vehicle\n",
    "    32: 0,  # motorcycle -> vehicle\n",
    "    33: 0   # bicycle -> vehicle\n",
    "}\n",
    "\n",
    "# Classes finales pour la visualisation\n",
    "CLASSES = {\n",
    "    0: \"vehicle\", \n",
    "    1: \"road\",\n",
    "    2: \"sidewalk\",\n",
    "    3: \"building\",\n",
    "    4: \"pole\",\n",
    "    5: \"traffic light\",\n",
    "    6: \"traffic sign\",\n",
    "    7: \"background\"\n",
    "}\n",
    "\n",
    "# Couleurs pour chaque classe\n",
    "COLORS = {\n",
    "    0: [0, 0, 142],     # vehicle - bleu foncé\n",
    "    1: [128, 64, 128],  # road - violet\n",
    "    2: [244, 35, 232],  # sidewalk - rose\n",
    "    3: [70, 70, 70],    # building - gris\n",
    "    4: [153, 153, 153], # pole - gris clair\n",
    "    5: [250, 170, 30],  # traffic light - orange\n",
    "    6: [220, 220, 0],   # traffic sign - jaune\n",
    "    7: [0, 0, 0]        # background - noir\n",
    "}\n",
    "\n",
    "# Fonction pour visualiser les images et masques\n",
    "def visualize_sample(image, mask, title=\"Sample\"):\n",
    "    \"\"\"\n",
    "    Visualise une image et son masque de segmentation\n",
    "    \"\"\"\n",
    "    # Créer une palette de couleurs pour les masques\n",
    "    colors = np.array([list(COLORS.values())])\n",
    "    \n",
    "    # Convertir le masque en image RGB\n",
    "    mask_rgb = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n",
    "    for class_idx, color in COLORS.items():\n",
    "        mask_rgb[mask == class_idx] = color\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"{title} - Image\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mask_rgb)\n",
    "    plt.title(f\"{title} - Mask\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Fonction pour réorganiser le dataset en respectant l'équilibre des classes\n",
    "def reorganize_dataset(source_dir, target_dir, test_size=0.2, val_size=0.15):\n",
    "    \"\"\"\n",
    "    Réorganise le dataset en regroupant toutes les images dans un seul dossier\n",
    "    et en les répartissant en train, val et test en respectant l'équilibre des classes\n",
    "    \"\"\"\n",
    "    # Créer les répertoires cibles s'ils n'existent pas\n",
    "    os.makedirs(os.path.join(target_dir, 'train', 'images'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(target_dir, 'train', 'masks'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(target_dir, 'val', 'images'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(target_dir, 'val', 'masks'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(target_dir, 'test', 'images'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(target_dir, 'test', 'masks'), exist_ok=True)\n",
    "    \n",
    "    # Trouver tous les fichiers de masque gt_labelIds\n",
    "    mask_files = []\n",
    "    for root, _, files in os.walk(source_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('labelIds.png'):\n",
    "                mask_files.append(os.path.join(root, file))\n",
    "    \n",
    "    # Analyse des masques pour déterminer la distribution des classes\n",
    "    print(f\"Total mask files found: {len(mask_files)}\")\n",
    "    \n",
    "    # Calculer la distribution des classes dans chaque masque\n",
    "    class_distribution = {}\n",
    "    for mask_file in tqdm(mask_files, desc=\"Analyzing masks\"):\n",
    "        mask = cv2.imread(mask_file, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # Remapper les classes selon notre mapping\n",
    "        remapped_mask = np.zeros_like(mask, dtype=np.uint8)\n",
    "        for original, target in CLASS_MAPPING.items():\n",
    "            remapped_mask[mask == original] = target\n",
    "            \n",
    "        # Compter les occurrences de chaque classe\n",
    "        unique_classes = np.unique(remapped_mask)\n",
    "        \n",
    "        # Créer une clé basée sur les classes présentes\n",
    "        class_key = '_'.join(map(str, sorted(unique_classes)))\n",
    "        \n",
    "        if class_key not in class_distribution:\n",
    "            class_distribution[class_key] = []\n",
    "        \n",
    "        class_distribution[class_key].append(mask_file)\n",
    "    \n",
    "    # Afficher la distribution des combinaisons de classes\n",
    "    print(\"\\nDistribution of class combinations:\")\n",
    "    for class_key, files in sorted(class_distribution.items(), key=lambda x: len(x[1]), reverse=True):\n",
    "        print(f\"Classes {class_key}: {len(files)} images\")\n",
    "    \n",
    "    # Diviser chaque groupe en train, val et test\n",
    "    train_masks = []\n",
    "    val_masks = []\n",
    "    test_masks = []\n",
    "    \n",
    "    for class_key, files in class_distribution.items():\n",
    "        # Premier split: train+val vs test\n",
    "        train_val, test_split = train_test_split(files, test_size=test_size, random_state=SEED)\n",
    "        \n",
    "        # Deuxième split: train vs val\n",
    "        train_split, val_split = train_test_split(train_val, test_size=val_size/(1-test_size), random_state=SEED)\n",
    "        \n",
    "        train_masks.extend(train_split)\n",
    "        val_masks.extend(val_split)\n",
    "        test_masks.extend(test_split)\n",
    "    \n",
    "    print(f\"\\nSplit results:\")\n",
    "    print(f\"Train: {len(train_masks)} images\")\n",
    "    print(f\"Validation: {len(val_masks)} images\")\n",
    "    print(f\"Test: {len(test_masks)} images\")\n",
    "    \n",
    "    # Copier les fichiers vers les répertoires cibles\n",
    "    def copy_files_to_target(mask_files, target_subdir):\n",
    "        for mask_file in tqdm(mask_files, desc=f\"Copying {target_subdir} files\"):\n",
    "            # Obtenir le nom de fichier sans extension\n",
    "            filename = os.path.basename(mask_file).replace('_gtFine_labelIds.png', '')\n",
    "            \n",
    "            # Trouver l'image correspondante\n",
    "            img_dir = os.path.dirname(mask_file).replace('gtFine', 'leftImg8bit')\n",
    "            img_file = os.path.join(img_dir, f\"{filename}_leftImg8bit.png\")\n",
    "            \n",
    "            if os.path.exists(img_file):\n",
    "                # Copier l'image et le masque\n",
    "                shutil.copy2(img_file, os.path.join(target_dir, target_subdir, 'images', f\"{filename}.png\"))\n",
    "                shutil.copy2(mask_file, os.path.join(target_dir, target_subdir, 'masks', f\"{filename}.png\"))\n",
    "            else:\n",
    "                print(f\"Warning: Could not find image for {mask_file}\")\n",
    "    \n",
    "    copy_files_to_target(train_masks, 'train')\n",
    "    copy_files_to_target(val_masks, 'val')\n",
    "    copy_files_to_target(test_masks, 'test')\n",
    "    \n",
    "    return len(train_masks), len(val_masks), len(test_masks)\n",
    "\n",
    "# Exécuter la réorganisation du dataset si nécessaire\n",
    "# Pour ce notebook, nous supposons que le dataset est déjà réorganisé\n",
    "# reorganize_dataset('path/to/cityscapes', 'path/to/processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Préparation des augmentations de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir différentes pipelines d'augmentation de données\n",
    "def get_basic_augmentation():\n",
    "    \"\"\"Augmentations de base: flip horizontal et vertical\"\"\"\n",
    "    return A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "\n",
    "def get_medium_augmentation():\n",
    "    \"\"\"Augmentations moyennes: flips, rotation, brightness/contrast\"\"\"\n",
    "    return A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.2),\n",
    "        A.RandomRotate90(p=0.2),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "\n",
    "def get_advanced_augmentation():\n",
    "    \"\"\"Augmentations avancées: ajout de transformations géométriques et d'intensité\"\"\"\n",
    "    return A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "        A.OneOf([\n",
    "            A.GridDistortion(p=1),\n",
    "            A.ElasticTransform(p=1),\n",
    "            A.OpticalDistortion(p=1)\n",
    "        ], p=0.3),\n",
    "        A.OneOf([\n",
    "            A.GaussNoise(p=1),\n",
    "            A.GaussianBlur(p=1),\n",
    "            A.MotionBlur(p=1),\n",
    "        ], p=0.2),\n",
    "        A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.2),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "\n",
    "# Visualiser les effets des augmentations\n",
    "def visualize_augmentations(image, mask, augmentation, n_examples=3):\n",
    "    \"\"\"\n",
    "    Visualise les effets des augmentations sur une image et son masque\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, n_examples*5))\n",
    "    \n",
    "    # Image et masque originaux\n",
    "    plt.subplot(n_examples+1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Convertir le masque en image RGB pour la visualisation\n",
    "    mask_rgb = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n",
    "    for class_idx, color in COLORS.items():\n",
    "        mask_rgb[mask == class_idx] = color\n",
    "        \n",
    "    plt.subplot(n_examples+1, 2, 2)\n",
    "    plt.imshow(mask_rgb)\n",
    "    plt.title('Original Mask')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Appliquer les augmentations\n",
    "    for i in range(n_examples):\n",
    "        # Appliquer l'augmentation\n",
    "        augmented = augmentation(image=image, mask=mask)\n",
    "        aug_image, aug_mask = augmented['image'], augmented['mask']\n",
    "        \n",
    "        # Convertir l'image augmentée pour l'affichage (dénormaliser)\n",
    "        if aug_image.dtype == np.float32:\n",
    "            aug_image_display = (aug_image * 255).astype(np.uint8)\n",
    "        else:\n",
    "            aug_image_display = aug_image\n",
    "        \n",
    "        # Convertir le masque augmenté en RGB\n",
    "        aug_mask_rgb = np.zeros((aug_mask.shape[0], aug_mask.shape[1], 3), dtype=np.uint8)\n",
    "        for class_idx, color in COLORS.items():\n",
    "            aug_mask_rgb[aug_mask == class_idx] = color\n",
    "        \n",
    "        # Afficher l'image augmentée\n",
    "        plt.subplot(n_examples+1, 2, 2*i+3)\n",
    "        plt.imshow(aug_image_display)\n",
    "        plt.title(f'Augmented Image {i+1}')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Afficher le masque augmenté\n",
    "        plt.subplot(n_examples+1, 2, 2*i+4)\n",
    "        plt.imshow(aug_mask_rgb)\n",
    "        plt.title(f'Augmented Mask {i+1}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Classe pour le générateur de données avec augmentation\n",
    "class SegmentationDataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"Générateur de données pour la segmentation avec support d'augmentation\"\"\"\n",
    "    \n",
    "    def __init__(self, image_dir, mask_dir, batch_size=8, img_size=(256, 256), \n",
    "                 augmentation=None, shuffle=True, remap_classes=True):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.augmentation = augmentation\n",
    "        self.shuffle = shuffle\n",
    "        self.remap_classes = remap_classes\n",
    "        \n",
    "        # Lister tous les fichiers image\n",
    "        self.image_files = sorted(glob.glob(os.path.join(image_dir, '*.png')))\n",
    "        \n",
    "        # Vérifier qu'il y a des images\n",
    "        if len(self.image_files) == 0:\n",
    "            raise ValueError(f\"No image files found in {image_dir}\")\n",
    "            \n",
    "        print(f\"Found {len(self.image_files)} images in {image_dir}\")\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Renvoie le nombre de batchs par epoch\"\"\"\n",
    "        return int(np.ceil(len(self.image_files) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Génère un batch de données\"\"\"\n",
    "        # Sélectionner les fichiers pour ce batch\n",
    "        batch_image_files = self.image_files[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        # Initialiser les tableaux pour les images et masques\n",
    "        batch_images = np.zeros((len(batch_image_files), *self.img_size, 3), dtype=np.float32)\n",
    "        batch_masks = np.zeros((len(batch_image_files), *self.img_size), dtype=np.uint8)\n",
    "        \n",
    "        for i, image_file in enumerate(batch_image_files):\n",
    "            # Charger l'image\n",
    "            image = cv2.imread(image_file)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Trouver et charger le masque correspondant\n",
    "            filename = os.path.basename(image_file)\n",
    "            mask_file = os.path.join(self.mask_dir, filename)\n",
    "            mask = cv2.imread(mask_file, cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            # Remapper les classes si nécessaire\n",
    "            if self.remap_classes:\n",
    "                remapped_mask = np.zeros_like(mask, dtype=np.uint8)\n",
    "                for original, target in CLASS_MAPPING.items():\n",
    "                    remapped_mask[mask == original] = target\n",
    "                mask = remapped_mask\n",
    "            \n",
    "            # Redimensionner l'image et le masque\n",
    "            image = cv2.resize(image, self.img_size[::-1])  # cv2 utilise (width, height)\n",
    "            mask = cv2.resize(mask, self.img_size[::-1], interpolation=cv2.INTER_NEAREST)\n",
    "            \n",
    "            # Appliquer l'augmentation si disponible\n",
    "            if self.augmentation:\n",
    "                augmented = self.augmentation(image=image, mask=mask)\n",
    "                image, mask = augmented['image'], augmented['mask']\n",
    "            else:\n",
    "                # Normaliser l'image même sans augmentation\n",
    "                image = image / 255.0\n",
    "            \n",
    "            # Stocker dans les tableaux\n",
    "            batch_images[i] = image\n",
    "            batch_masks[i] = mask\n",
    "        \n",
    "        # Convertir les masques en one-hot encoding\n",
    "        batch_masks_one_hot = tf.keras.utils.to_categorical(batch_masks, num_classes=NUM_CLASSES)\n",
    "        \n",
    "        return batch_images, batch_masks_one_hot\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Mélanger les indices à la fin de chaque epoch\"\"\"\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.image_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Modèle UNet Mini\n",
    "def build_unet_mini(input_shape=(256, 256, 3), num_classes=8):\n",
    "    \"\"\"Construit un modèle UNet léger\"\"\"\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    conv1 = tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(conv2)\n",
    "    pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    conv3 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(conv3)\n",
    "    pool3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    # Bridge\n",
    "    conv4 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')(pool3)\n",
    "    conv4 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')(conv4)\n",
    "    \n",
    "    # Decoder\n",
    "    up5 = tf.keras.layers.UpSampling2D(size=(2, 2))(conv4)\n",
    "    concat5 = tf.keras.layers.Concatenate()([up5, conv3])\n",
    "    conv5 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(concat5)\n",
    "    conv5 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(conv5)\n",
    "    \n",
    "    up6 = tf.keras.layers.UpSampling2D(size=(2, 2))(conv5)\n",
    "    concat6 = tf.keras.layers.Concatenate()([up6, conv2])\n",
    "    conv6 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(concat6)\n",
    "    conv6 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(conv6)\n",
    "    \n",
    "    up7 = tf.keras.layers.UpSampling2D(size=(2, 2))(conv6)\n",
    "    concat7 = tf.keras.layers.Concatenate()([up7, conv1])\n",
    "    conv7 = tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same')(concat7)\n",
    "    conv7 = tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same')(conv7)\n",
    "    \n",
    "    # Output\n",
    "    outputs = tf.keras.layers.Conv2D(num_classes, 1, activation='softmax')(conv7)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# 2. Modèle VGG16-UNet avec encoder pré-entraîné\n",
    "def build_vgg16_unet(input_shape=(256, 256, 3), num_classes=8, trainable_encoder=False):\n",
    "    \"\"\"Construit un modèle UNet avec un encoder VGG16 pré-entraîné\"\"\"\n",
    "    # Charger le modèle VGG16 pré-entraîné\n",
    "    base_model = tf.keras.applications.VGG16(\n",
    "        include_top=False, \n",
    "        weights='imagenet',\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    \n",
    "    # Figer les poids du modèle de base\n",
    "    base_model.trainable = trainable_encoder\n",
    "    \n",
    "    # Extraire les sorties des couches pour les skip connections\n",
    "    layer_names = [\n",
    "        'block1_conv2',   # 64 filtres\n",
    "        'block2_conv2',   # 128 filtres\n",
    "        'block3_conv3',   # 256 filtres\n",
    "        'block4_conv3',   # 512 filtres\n",
    "        'block5_conv3'    # 512 filtres\n",
    "    ]\n",
    "    \n",
    "    layers = [base_model.get_layer(name).output for name in layer_names]\n",
    "    \n",
    "    # Créer le modèle d'extraction de features\n",
    "    encoder = tf.keras.models.Model(inputs=base_model.input, outputs=layers)\n",
    "    \n",
    "    # Entrée\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Appliquer l'encoder\n",
    "    skips = encoder(inputs)\n",
    "    \n",
    "    # Bridge est la sortie de l'encoder\n",
    "    x = skips[-1]\n",
    "    \n",
    "    # Decoder avec skip connections\n",
    "    for i in range(len(skips)-2, -1, -1):\n",
    "        x = tf.keras.layers.UpSampling2D(2)(x)\n",
    "        concat = tf.keras.layers.Concatenate()([x, skips[i]])\n",
    "        \n",
    "        x = tf.keras.layers.Conv2D(512 // 2**(len(skips)-i-1), 3, activation='relu', padding='same')(concat)\n",
    "        x = tf.keras.layers.Conv2D(512 // 2**(len(skips)-i-1), 3, activation='relu', padding='same')(x)\n",
    "    \n",
    "    # Couche de sortie\n",
    "    outputs = tf.keras.layers.Conv2D(num_classes, 1, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# 3. Modèle ResNet50-UNet avec encoder pré-entraîné\n",
    "def build_resnet50_unet(input_shape=(256, 256, 3), num_classes=8, trainable_encoder=False):\n",
    "    \"\"\"Construit un modèle UNet avec un encoder ResNet50 pré-entraîné\"\"\"\n",
    "    # Charger le modèle ResNet50 pré-entraîné\n",
    "    base_model = tf.keras.applications.ResNet50(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    \n",
    "    # Figer les poids du modèle de base\n",
    "    base_model.trainable = trainable_encoder\n",
    "    \n",
    "    # Extraire les sorties des couches pour les skip connections\n",
    "    layer_names = [\n",
    "        'conv1_relu',      # 64 filtres\n",
    "        'conv2_block3_out', # 256 filtres\n",
    "        'conv3_block4_out', # 512 filtres\n",
    "        'conv4_block6_out', # 1024 filtres\n",
    "        'conv5_block3_out'  # 2048 filtres\n",
    "    ]\n",
    "    \n",
    "    layers = [base_model.get_layer(name).output for name in layer_names]\n",
    "    \n",
    "    # Créer le modèle d'extraction de features\n",
    "    encoder = tf.keras.models.Model(inputs=base_model.input, outputs=layers)\n",
    "    \n",
    "    # Entrée\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Appliquer l'encoder\n",
    "    skips = encoder(inputs)\n",
    "    \n",
    "    # Bridge est la sortie de l'encoder\n",
    "    x = skips[-1]\n",
    "    \n",
    "    # Decoder avec skip connections\n",
    "    decoder_filters = [1024, 512, 256, 64, 32]  # Réduire progressivement le nombre de filtres\n",
    "    \n",
    "    for i in range(len(skips)-2, -1, -1):\n",
    "        x = tf.keras.layers.UpSampling2D(2)(x)\n",
    "        x = tf.keras.layers.Conv2D(decoder_filters[len(skips)-i-2], 3, activation='relu', padding='same')(x)\n",
    "        \n",
    "        # Skip connection\n",
    "        concat = tf.keras.layers.Concatenate()([x, skips[i]])\n",
    "        \n",
    "        # Double convolution\n",
    "        x = tf.keras.layers.Conv2D(decoder_filters[len(skips)-i-2], 3, activation='relu', padding='same')(concat)\n",
    "        x = tf.keras.layers.Conv2D(decoder_filters[len(skips)-i-2], 3, activation='relu', padding='same')(x)\n",
    "    \n",
    "    # Dernière couche de upsampling et convolution\n",
    "    x = tf.keras.layers.UpSampling2D(2)(x)\n",
    "    x = tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same')(x)\n",
    "    \n",
    "    # Couche de sortie\n",
    "    outputs = tf.keras.layers.Conv2D(num_classes, 1, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Métriques et fonctions de perte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des métriques et fonctions de perte pour la segmentation\n",
    "\n",
    "# Fonction IoU (Intersection over Union) / Indice de Jaccard\n",
    "def jaccard_index(y_true, y_pred, smooth=1e-5):\n",
    "    \"\"\"\n",
    "    Calcule l'indice de Jaccard (IoU) entre les prédictions et les vérités terrain\n",
    "    \"\"\"\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    union = tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) - intersection\n",
    "    return (intersection + smooth) / (union + smooth)\n",
    "\n",
    "# Coefficient Dice (F1-score)\n",
    "def dice_coefficient(y_true, y_pred, smooth=1e-5):\n",
    "    \"\"\"\n",
    "    Calcule le coefficient Dice entre les prédictions et les vérités terrain\n",
    "    \"\"\"\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
    "\n",
    "# Dice Loss\n",
    "def dice_loss(y_true, y_pred, smooth=1e-5):\n",
    "    \"\"\"\n",
    "    Calcule la perte Dice\n",
    "    \"\"\"\n",
    "    return 1 - dice_coefficient(y_true, y_pred, smooth)\n",
    "\n",
    "# Combinaison de BCE et Dice Loss\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Combine la perte BCE (Binary Cross Entropy) et la perte Dice\n",
    "    \"\"\"\n",
    "    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    dice = dice_loss(y_true, y_pred)\n",
    "    return bce + dice\n",
    "\n",
    "# Fonction de perte pondérée pour gérer le déséquilibre des classes\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    Génère une fonction de perte d'entropie croisée catégorielle pondérée\n",
    "    pour gérer le déséquilibre des classes\n",
    "    \"\"\"\n",
    "    weights = tf.keras.backend.variable(weights)\n",
    "    \n",
    "    def loss(y_true, y_pred):\n",
    "        # Scale predictions so that the class probabilities of each sample sum to 1\n",
    "        y_pred /= tf.keras.backend.sum(y_pred, axis=-1, keepdims=True)\n",
    "        \n",
    "        # Clip to prevent NaN's and Inf's\n",
    "        y_pred = tf.keras.backend.clip(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "        \n",
    "        # Calculate the weighted cross-entropy\n",
    "        loss = y_true * tf.keras.backend.log(y_pred) * weights\n",
    "        loss = -tf.keras.backend.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Focal Loss pour gérer les classes déséquilibrées\n",
    "def categorical_focal_loss(gamma=2.0, alpha=None):\n",
    "    \"\"\"\n",
    "    Implémentation de la Focal Loss pour la segmentation multi-classes\n",
    "    \"\"\"\n",
    "    def focal_loss(y_true, y_pred):\n",
    "        # Scale predictions so that the class probabilities of each sample sum to 1\n",
    "        y_pred /= tf.keras.backend.sum(y_pred, axis=-1, keepdims=True)\n",
    "        \n",
    "        # Clip to prevent NaN's and Inf's\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.keras.backend.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        # Calculate focal loss\n",
    "        loss = -y_true * tf.keras.backend.pow(1 - y_pred, gamma) * tf.keras.backend.log(y_pred)\n",
    "        \n",
    "        # Apply class weights if provided\n",
    "        if alpha is not None:\n",
    "            loss = alpha * loss\n",
    "            \n",
    "        return tf.keras.backend.sum(loss, axis=-1)\n",
    "    \n",
    "    return focal_loss\n",
    "\n",
    "# Classe pour le Mean IoU en tant que métrique Keras\n",
    "class MeanIoU(tf.keras.metrics.Metric):\n",
    "    def __init__(self, num_classes, name='mean_iou', **kwargs):\n",
    "        super(MeanIoU, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.total_iou = self.add_weight('total_iou', initializer='zeros')\n",
    "        self.count = self.add_weight('count', initializer='zeros')\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.argmax(y_true, axis=-1)\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        \n",
    "        current_iou = 0\n",
    "        \n",
    "        # Calculer IoU pour chaque classe et faire la moyenne\n",
    "        for i in range(self.num_classes):\n",
    "            y_true_class = tf.cast(tf.equal(y_true, i), tf.float32)\n",
    "            y_pred_class = tf.cast(tf.equal(y_pred, i), tf.float32)\n",
    "            \n",
    "            intersection = tf.reduce_sum(y_true_class * y_pred_class)\n",
    "            union = tf.reduce_sum(y_true_class) + tf.reduce_sum(y_pred_class) - intersection\n",
    "            \n",
    "            # Éviter la division par zéro\n",
    "            iou = tf.cond(\n",
    "                tf.equal(union, 0),\n",
    "                lambda: tf.constant(1.0),  # Si union=0, IoU=1 (parfait)\n",
    "                lambda: intersection / union\n",
    "            )\n",
    "            \n",
    "            current_iou += iou\n",
    "        \n",
    "        current_iou /= tf.cast(self.num_classes, tf.float32)\n",
    "        \n",
    "        self.total_iou.assign_add(current_iou)\n",
    "        self.count.assign_add(1)\n",
    "        \n",
    "    def result(self):\n",
    "        return self.total_iou / self.count\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.total_iou.assign(0)\n",
    "        self.count.assign(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entraînement et évaluation des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour entraîner et évaluer un modèle\n",
    "def train_and_evaluate(model, model_name, train_gen, val_gen, test_gen, \n",
    "                       loss_function, epochs=30, learning_rate=1e-4,\n",
    "                       metrics=['accuracy', 'MeanIoU'], early_stopping_patience=10):\n",
    "    \"\"\"\n",
    "    Entraîne et évalue un modèle de segmentation\n",
    "    \n",
    "    Args:\n",
    "        model: Modèle Keras à entraîner\n",
    "        model_name: Nom du modèle pour sauvegarder les checkpoints\n",
    "        train_gen: Générateur d'entraînement\n",
    "        val_gen: Générateur de validation\n",
    "        test_gen: Générateur de test\n",
    "        loss_function: Fonction de perte à utiliser\n",
    "        epochs: Nombre maximum d'epochs\n",
    "        learning_rate: Taux d'apprentissage\n",
    "        metrics: Liste des métriques à suivre\n",
    "        early_stopping_patience: Patience pour l'early stopping\n",
    "    \n",
    "    Returns:\n",
    "        history: Historique d'entraînement\n",
    "        evaluation: Résultats d'évaluation sur l'ensemble test\n",
    "        training_time: Temps d'entraînement en secondes\n",
    "    \"\"\"\n",
    "    # Configurer les métriques\n",
    "    metrics_list = []\n",
    "    if 'accuracy' in metrics:\n",
    "        metrics_list.append('accuracy')\n",
    "    if 'MeanIoU' in metrics:\n",
    "        metrics_list.append(MeanIoU(num_classes=NUM_CLASSES))\n",
    "    \n",
    "    # Compiler le modèle\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=loss_function,\n",
    "        metrics=metrics_list\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    checkpoint_path = f\"checkpoints/{model_name}_best.h5\"\n",
    "    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "    \n",
    "    callbacks = [\n",
    "        # Sauvegarder le meilleur modèle\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            checkpoint_path,\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            mode='min',\n",
    "            verbose=1\n",
    "        ),\n",
    "        # Réduire le taux d'apprentissage quand la validation loss stagne\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        ),\n",
    "        # Arrêter l'entraînement si la validation loss ne s'améliore pas\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=early_stopping_patience,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        # Logging TensorBoard\n",
    "        tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=f'logs/{model_name}_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}',\n",
    "            histogram_freq=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Entraîner le modèle\n",
    "    print(f\"\\nEntraînement du modèle {model_name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\nTemps d'entraînement: {training_time:.2f} secondes\")\n",
    "    \n",
    "    # Charger le meilleur modèle\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"Chargement du meilleur modèle depuis {checkpoint_path}\")\n",
    "        model = tf.keras.models.load_model(\n",
    "            checkpoint_path,\n",
    "            custom_objects={\n",
    "                'MeanIoU': MeanIoU,\n",
    "                'dice_coefficient': dice_coefficient,\n",
    "                'jaccard_index': jaccard_index,\n",
    "                'dice_loss': dice_loss,\n",
    "                'bce_dice_loss': bce_dice_loss\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    # Évaluer le modèle sur l'ensemble de test\n",
    "    print(f\"\\nÉvaluation du modèle {model_name} sur l'ensemble de test...\")\n",
    "    evaluation = model.evaluate(test_gen, verbose=1)\n",
    "    \n",
    "    eval_result = {}\n",
    "    for i, metric in enumerate(model.metrics_names):\n",
    "        eval_result[metric] = evaluation[i]\n",
    "        print(f\"{metric}: {evaluation[i]:.4f}\")\n",
    "    \n",
    "    return history, eval_result, training_time, model\n",
    "\n",
    "# Fonction pour visualiser les prédictions du modèle\n",
    "def visualize_predictions(model, test_gen, num_samples=3):\n",
    "    \"\"\"\n",
    "    Visualise les prédictions du modèle sur quelques échantillons de test\n",
    "    \"\"\"\n",
    "    # Obtenir quelques échantillons de test\n",
    "    for i in range(min(num_samples, len(test_gen))):\n",
    "        images, masks_true = test_gen.__getitem__(i)\n",
    "        \n",
    "        # Ne prendre que la première image du batch\n",
    "        image = images[0]\n",
    "        mask_true = masks_true[0]\n",
    "        \n",
    "        # Faire une prédiction\n",
    "        mask_pred = model.predict(np.expand_dims(image, axis=0))[0]\n",
    "        \n",
    "        # Convertir les masques one-hot en masques de classe\n",
    "        mask_true_class = np.argmax(mask_true, axis=-1)\n",
    "        mask_pred_class = np.argmax(mask_pred, axis=-1)\n",
    "        \n",
    "        # Créer des masques RGB pour la visualisation\n",
    "        mask_true_rgb = np.zeros((*mask_true_class.shape, 3), dtype=np.uint8)\n",
    "        mask_pred_rgb = np.zeros((*mask_pred_class.shape, 3), dtype=np.uint8)\n",
    "        \n",
    "        for class_idx, color in COLORS.items():\n",
    "            mask_true_rgb[mask_true_class == class_idx] = color\n",
    "            mask_pred_rgb[mask_pred_class == class_idx] = color\n",
    "        \n",
    "        # Créer une figure pour afficher l'image, la vérité terrain et la prédiction\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        # L'image a été normalisée, la renormaliser pour l'affichage\n",
    "        plt.imshow(image)\n",
    "        plt.title('Image')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(mask_true_rgb)\n",
    "        plt.title('Vérité terrain')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(mask_pred_rgb)\n",
    "        plt.title('Prédiction')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "# Fonction pour visualiser l'historique d'entraînement\n",
    "def plot_training_history(history, model_name):\n",
    "    \"\"\"\n",
    "    Visualise l'historique d'entraînement d'un modèle\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{model_name} - Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Metrics (accuracy or IoU)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    \n",
    "    if 'accuracy' in history.history:\n",
    "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title(f'{model_name} - Accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "    \n",
    "    if 'mean_iou' in history.history:\n",
    "        plt.plot(history.history['mean_iou'], label='Training IoU')\n",
    "        plt.plot(history.history['val_mean_iou'], label='Validation IoU')\n",
    "        plt.title(f'{model_name} - Mean IoU')\n",
    "        plt.ylabel('IoU')\n",
    "    \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'results/{model_name}_history.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Fonction pour comparer les performances des différents modèles\n",
    "def compare_models(results):\n",
    "    \"\"\"\n",
    "    Compare les performances de différents modèles\n",
    "    \n",
    "    Args:\n",
    "        results: Liste de dictionnaires contenant les résultats pour chaque modèle\n",
    "    \"\"\"\n",
    "    # Extraire les noms des modèles et les métriques\n",
    "    model_names = [result['model_name'] for result in results]\n",
    "    loss_values = [result['evaluation']['loss'] for result in results]\n",
    "    \n",
    "    # Chercher les métriques communes\n",
    "    metrics = set()\n",
    "    for result in results:\n",
    "        metrics.update(result['evaluation'].keys())\n",
    "    \n",
    "    metrics = sorted(list(metrics - {'loss'}))\n",
    "    \n",
    "    # Créer un DataFrame pour les résultats\n",
    "    df_results = pd.DataFrame({\n",
    "        'Model': model_names,\n",
    "        'Loss': loss_values,\n",
    "        'Training Time (s)': [result['training_time'] for result in results]\n",
    "    })\n",
    "    \n",
    "    # Ajouter les métriques\n",
    "    for metric in metrics:\n",
    "        df_results[metric] = [result['evaluation'].get(metric, float('nan')) for result in results]\n",
    "    \n",
    "    # Afficher le tableau des résultats\n",
    "    print(\"\\n=== Comparaison des modèles ===\")\n",
    "    print(df_results.to_string(index=False))\n",
    "    \n",
    "    # Sauvegarder le tableau des résultats\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "    df_results.to_csv('results/model_comparison.csv', index=False)\n",
    "    \n",
    "    # Créer un graphique comparatif\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Comparer les métriques\n",
    "    n_metrics = len(metrics) + 1  # +1 pour la loss\n",
    "    n_cols = 2\n",
    "    n_rows = (n_metrics + 1) // n_cols\n",
    "    \n",
    "    # Loss\n",
    "    plt.subplot(n_rows, n_cols, 1)\n",
    "    sns.barplot(x='Model', y='Loss', data=df_results)\n",
    "    plt.title('Loss')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Autres métriques\n",
    "    for i, metric in enumerate(metrics):\n",
    "        plt.subplot(n_rows, n_cols, i+2)\n",
    "        sns.barplot(x='Model', y=metric, data=df_results)\n",
    "        plt.title(metric)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Temps d'entraînement\n",
    "    plt.subplot(n_rows, n_cols, n_metrics+1)\n",
    "    sns.barplot(x='Model', y='Training Time (s)', data=df_results)\n",
    "    plt.title('Training Time (s)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/model_comparison.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Retourner le meilleur modèle selon la métrique IoU\n",
    "    if 'mean_iou' in metrics:\n",
    "        best_idx = df_results['mean_iou'].idxmax()\n",
    "    elif 'accuracy' in metrics:\n",
    "        best_idx = df_results['accuracy'].idxmax()\n",
    "    else:\n",
    "        best_idx = df_results['Loss'].idxmin()\n",
    "    \n",
    "    best_model = {\n",
    "        'name': model_names[best_idx],\n",
    "        'model': results[best_idx]['model']\n",
    "    }\n",
    "    \n",
    "    return df_results, best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expérimentations et comparaison des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin vers le dataset organisé\n",
    "dataset_path = 'processed_dataset'  # Adaptez selon votre structure de fichiers\n",
    "img_size = (256, 256)\n",
    "batch_size = 8\n",
    "\n",
    "# Créer les générateurs de données avec différentes augmentations\n",
    "train_gen_basic = SegmentationDataGenerator(\n",
    "    os.path.join(dataset_path, 'train', 'images'),\n",
    "    os.path.join(dataset_path, 'train', 'masks'),\n",
    "    batch_size=batch_size,\n",
    "    img_size=img_size,\n",
    "    augmentation=get_basic_augmentation()\n",
    ")\n",
    "\n",
    "train_gen_medium = SegmentationDataGenerator(\n",
    "    os.path.join(dataset_path, 'train', 'images'),\n",
    "    os.path.join(dataset_path, 'train', 'masks'),\n",
    "    batch_size=batch_size,\n",
    "    img_size=img_size,\n",
    "    augmentation=get_medium_augmentation()\n",
    ")\n",
    "\n",
    "train_gen_advanced = SegmentationDataGenerator(\n",
    "    os.path.join(dataset_path, 'train', 'images'),\n",
    "    os.path.join(dataset_path, 'train', 'masks'),\n",
    "    batch_size=batch_size,\n",
    "    img_size=img_size,\n",
    "    augmentation=get_advanced_augmentation()\n",
    ")\n",
    "\n",
    "val_gen = SegmentationDataGenerator(\n",
    "    os.path.join(dataset_path, 'val', 'images'),\n",
    "    os.path.join(dataset_path, 'val', 'masks'),\n",
    "    batch_size=batch_size,\n",
    "    img_size=img_size,\n",
    "    augmentation=None,  # Pas d'augmentation pour la validation\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_gen = SegmentationDataGenerator(\n",
    "    os.path.join(dataset_path, 'test', 'images'),\n",
    "    os.path.join(dataset_path, 'test', 'masks'),\n",
    "    batch_size=batch_size,\n",
    "    img_size=img_size,\n",
    "    augmentation=None,  # Pas d'augmentation pour le test\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Visualiser les exemples d'augmentation\n",
    "try:\n",
    "    # Obtenir un exemple d'image et de masque\n",
    "    images, masks = train_gen_basic.__getitem__(0)\n",
    "    image, mask = images[0], np.argmax(masks[0], axis=-1)\n",
    "    \n",
    "    # Visualiser les effets des différentes augmentations\n",
    "    print(\"Augmentation basique:\")\n",
    "    visualize_augmentations(image, mask, get_basic_augmentation(), n_examples=2)\n",
    "    \n",
    "    print(\"Augmentation moyenne:\")\n",
    "    visualize_augmentations(image, mask, get_medium_augmentation(), n_examples=2)\n",
    "    \n",
    "    print(\"Augmentation avancée:\")\n",
    "    visualize_augmentations(image, mask, get_advanced_augmentation(), n_examples=2)\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors de la visualisation des augmentations: {e}\")\n",
    "\n",
    "# Liste pour stocker les résultats\n",
    "results = []\n",
    "\n",
    "# Expérience 1: UNet Mini avec augmentation basique et Dice Loss\n",
    "print(\"\\n=== Expérience 1: UNet Mini avec augmentation basique et Dice Loss ===\")\n",
    "model1 = build_unet_mini(input_shape=(*img_size, 3), num_classes=NUM_CLASSES)\n",
    "model1.summary()\n",
    "\n",
    "history1, eval1, time1, model1 = train_and_evaluate(\n",
    "    model=model1,\n",
    "    model_name='unet_mini_basic_dice',\n",
    "    train_gen=train_gen_basic,\n",
    "    val_gen=val_gen,\n",
    "    test_gen=test_gen,\n",
    "    loss_function=dice_loss,\n",
    "    epochs=30,\n",
    "    learning_rate=1e-4\n",
    ")\n",
    "\n",
    "plot_training_history(history1, 'UNet Mini (Basic Aug, Dice Loss)')\n",
    "visualize_predictions(model1, test_gen, num_samples=2)\n",
    "\n",
    "results.append({\n",
    "    'model_name': 'UNet Mini (Basic Aug, Dice Loss)',\n",
    "    'evaluation': eval1,\n",
    "    'training_time': time1,\n",
    "    'model': model1\n",
    "})\n",
    "\n",
    "# Expérience 2: UNet Mini avec augmentation avancée et BCE-Dice Loss\n",
    "print(\"\\n=== Expérience 2: UNet Mini avec augmentation avancée et BCE-Dice Loss ===\")\n",
    "model2 = build_unet_mini(input_shape=(*img_size, 3), num_classes=NUM_CLASSES)\n",
    "\n",
    "history2, eval2, time2, model2 = train_and_evaluate(\n",
    "    model=model2,\n",
    "    model_name='unet_mini_advanced_bce_dice',\n",
    "    train_gen=train_gen_advanced,\n",
    "    val_gen=val_gen,\n",
    "    test_gen=test_gen,\n",
    "    loss_function=bce_dice_loss,\n",
    "    epochs=30,\n",
    "    learning_rate=1e-4\n",
    ")\n",
    "\n",
    "plot_training_history(history2, 'UNet Mini (Advanced Aug, BCE-Dice Loss)')\n",
    "visualize_predictions(model2, test_gen, num_samples=2)\n",
    "\n",
    "results.append({\n",
    "    'model_name': 'UNet Mini (Advanced Aug, BCE-Dice Loss)',\n",
    "    'evaluation': eval2,\n",
    "    'training_time': time2,\n",
    "    'model': model2\n",
    "})\n",
    "\n",
    "# Expérience 3: VGG16-UNet avec augmentation moyenne et Focal Loss\n",
    "print(\"\\n=== Expérience 3: VGG16-UNet avec augmentation moyenne et Focal Loss ===\")\n",
    "model3 = build_vgg16_unet(input_shape=(*img_size, 3), num_classes=NUM_CLASSES)\n",
    "model3.summary()\n",
    "\n",
    "# Calculer les poids de classe pour le Focal Loss (optionnel)\n",
    "class_weights = None  # Vous pouvez calculer des poids basés sur la distribution des classes\n",
    "\n",
    "history3, eval3, time3, model3 = train_and_evaluate(\n",
    "    model=model3,\n",
    "    model_name='vgg16_unet_medium_focal',\n",
    "    train_gen=train_gen_medium,\n",
    "    val_gen=val_gen,\n",
    "    test_gen=test_gen,\n",
    "    loss_function=categorical_focal_loss(gamma=2.0, alpha=class_weights),\n",
    "    epochs=30,\n",
    "    learning_rate=5e-5  # Plus faible pour le modèle pré-entraîné\n",
    ")\n",
    "\n",
    "plot_training_history(history3, 'VGG16-UNet (Medium Aug, Focal Loss)')\n",
    "visualize_predictions(model3, test_gen, num_samples=2)\n",
    "\n",
    "results.append({\n",
    "    'model_name': 'VGG16-UNet (Medium Aug, Focal Loss)',\n",
    "    'evaluation': eval3,\n",
    "    'training_time': time3,\n",
    "    'model': model3\n",
    "})\n",
    "\n",
    "# Expérience 4: ResNet50-UNet avec augmentation avancée et BCE-Dice Loss\n",
    "print(\"\\n=== Expérience 4: ResNet50-UNet avec augmentation avancée et BCE-Dice Loss ===\")\n",
    "model4 = build_resnet50_unet(input_shape=(*img_size, 3), num_classes=NUM_CLASSES)\n",
    "model4.summary()\n",
    "\n",
    "history4, eval4, time4, model4 = train_and_evaluate(\n",
    "    model=model4,\n",
    "    model_name='resnet50_unet_advanced_bce_dice',\n",
    "    train_gen=train_gen_advanced,\n",
    "    val_gen=val_gen,\n",
    "    test_gen=test_gen,\n",
    "    loss_function=bce_dice_loss,\n",
    "    epochs=30,\n",
    "    learning_rate=5e-5  # Plus faible pour le modèle pré-entraîné\n",
    ")\n",
    "\n",
    "plot_training_history(history4, 'ResNet50-UNet (Advanced Aug, BCE-Dice Loss)')\n",
    "visualize_predictions(model4, test_gen, num_samples=2)\n",
    "\n",
    "results.append({\n",
    "    'model_name': 'ResNet50-UNet (Advanced Aug, BCE-Dice Loss)',\n",
    "    'evaluation': eval4,\n",
    "    'training_time': time4,\n",
    "    'model': model4\n",
    "})\n",
    "\n",
    "# Comparer les modèles\n",
    "df_comparison, best_model = compare_models(results)\n",
    "\n",
    "print(f\"\\nMeilleur modèle: {best_model['name']}\")\n",
    "\n",
    "# Sauvegarder le meilleur modèle\n",
    "best_model['model'].save(f'best_model_{best_model[\"name\"]}.h5')\n",
    "print(f\"Meilleur modèle sauvegardé sous 'best_model_{best_model['name']}.h5'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmentation optimisée sur de nouvelles images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour segmenter une nouvelle image avec le meilleur modèle\n",
    "def segment_image(image_path, model, img_size=(256, 256)):\n",
    "    \"\"\"\n",
    "    Segmente une image avec le modèle fourni\n",
    "    \n",
    "    Args:\n",
    "        image_path: Chemin vers l'image à segmenter\n",
    "        model: Modèle à utiliser pour la segmentation\n",
    "        img_size: Taille de l'image pour le modèle\n",
    "    \n",
    "    Returns:\n",
    "        Image originale et masque de segmentation\n",
    "    \"\"\"\n",
    "    # Charger l'image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Redimensionner l'image\n",
    "    image_resized = cv2.resize(image, img_size[::-1])\n",
    "    \n",
    "    # Normaliser l'image\n",
    "    image_norm = image_resized / 255.0\n",
    "    \n",
    "    # Faire la prédiction\n",
    "    mask_pred = model.predict(np.expand_dims(image_norm, axis=0))[0]\n",
    "    mask_pred_class = np.argmax(mask_pred, axis=-1)\n",
    "    \n",
    "    # Créer un masque RGB pour la visualisation\n",
    "    mask_pred_rgb = np.zeros((*mask_pred_class.shape, 3), dtype=np.uint8)\n",
    "    \n",
    "    for class_idx, color in COLORS.items():\n",
    "        mask_pred_rgb[mask_pred_class == class_idx] = color\n",
    "    \n",
    "    # Afficher l'image et la segmentation\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image_resized)\n",
    "    plt.title('Image originale')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mask_pred_rgb)\n",
    "    plt.title('Segmentation')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return image_resized, mask_pred_rgb\n",
    "\n",
    "# Exemple d'utilisation sur de nouvelles images\n",
    "test_images = [\n",
    "    'test_images/image1.jpg',\n",
    "    'test_images/image2.jpg',\n",
    "    'test_images/image3.jpg'\n",
    "]\n",
    "\n",
    "# Commenter ce bloc si les images de test n'existent pas\n",
    "for image_path in test_images:\n",
    "    try:\n",
    "        print(f\"Segmentation de {image_path}:\")\n",
    "        segment_image(image_path, best_model['model'], img_size)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la segmentation de {image_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion et perspectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les résultats finaux et les conclusions\n",
    "print(\"\\n=== Résultats finaux ===\")\n",
    "print(df_comparison)\n",
    "\n",
    "# Réfléchir aux améliorations possibles\n",
    "improvements = [\n",
    "    \"Utiliser une résolution d'image plus élevée (512x512 ou plus)\",\n",
    "    \"Tester d'autres architectures comme DeepLabV3+, PSPNet, ou HRNet\",\n",
    "    \"Implémenter des techniques d'apprentissage par transfert plus avancées\",\n",
    "    \"Utiliser des techniques d'augmentation de données plus spécifiques au domaine urbain\",\n",
    "    \"Explorer l'optimisation des hyperparamètres avec Optuna ou Ray Tune\",\n",
    "    \"Combiner différentes fonctions de perte pour mieux gérer les classes minoritaires\",\n",
    "    \"Utiliser l'ensemble learning en combinant plusieurs modèles\"\n",
    "]\n",
    "\n",
    "print(\"\\n=== Perspectives d'amélioration ===\")\n",
    "for i, improvement in enumerate(improvements, 1):\n",
    "    print(f\"{i}. {improvement}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
